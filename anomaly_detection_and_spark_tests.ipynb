{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import findspark\n",
    "findspark.init()\n",
    "#from pyspark import SparkContext, SparkConf, sql\n",
    "import pyspark as spark\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as spark_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "from pdb import set_trace as bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = spark.SparkConf().setAppName(\"Anomaly\").setMaster(\"local[2]\")\n",
    "sc = spark.SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_context = spark.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sql_context.read.csv(path='/home/l7/Downloads/UPM.HE.csv')\n",
    "df_keys = list(df.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df.filter((df._c0 != 'Date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Date='2000-01-03', Open='20.575001', High='22.000000', Low='20.500000', Close='21.100000', Adj Close='6.877053', Volume='601912')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = sql_context.createDataFrame(data=df_filtered.collect(),\n",
    "                                    schema=df.head()[:],\n",
    "                                    )\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overwriting column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Date', 'date'),\n",
       " ('Open', 'float'),\n",
       " ('High', 'float'),\n",
       " ('Low', 'float'),\n",
       " ('Close', 'float'),\n",
       " ('Adj Close', 'float'),\n",
       " ('Volume', 'float')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = df_new.withColumn('Date', df_new['Date'].cast(spark.sql.types.DateType()))\n",
    "for column in df_new.columns[1:]:\n",
    "    df_new = df_new.withColumn(column, df_new[column].cast(spark.sql.types.FloatType()))\n",
    "df_new = df_new.dropna()\n",
    "df_new.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.withColumn('h_l_diff', spark_functions.abs(df_new['High'] - df_new['Low']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+----+-----+---------+---------+---------+\n",
      "|      Date|  Open| High| Low|Close|Adj Close|   Volume| h_l_diff|\n",
      "+----------+------+-----+----+-----+---------+---------+---------+\n",
      "|2000-01-03|20.575| 22.0|20.5| 21.1| 6.877053| 601912.0|      1.5|\n",
      "|2000-01-04|  21.1|21.55|20.1|21.55|  7.02372|1412912.0|1.4499989|\n",
      "+----------+------+-----+----+-----+---------+---------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|              Open|         Adj Close|\n",
      "+-------+------------------+------------------+\n",
      "|  count|              4937|              4937|\n",
      "|   mean|15.573755305330595|10.106034078118963|\n",
      "| stddev| 5.505172848734595| 6.197554401683743|\n",
      "|    min|              4.44|          2.682938|\n",
      "|    max|             34.42|          32.96963|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.describe(['Open', 'Adj Close']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = spark.sql.window.Window.orderBy('Date')\n",
    "df_lag = df_new.withColumn('prev_day_price',\n",
    "                          spark_functions.lag(df_new['Adj Close'],count=1)\n",
    "                           .over(window_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns(present, past):\n",
    "    return (present - past) / past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+----+-----+---------+---------+---------+--------------+--------------------+\n",
      "|      Date|  Open| High| Low|Close|Adj Close|   Volume| h_l_diff|prev_day_price|       daily_returns|\n",
      "+----------+------+-----+----+-----+---------+---------+---------+--------------+--------------------+\n",
      "|2000-01-03|20.575| 22.0|20.5| 21.1| 6.877053| 601912.0|      1.5|          null|                null|\n",
      "|2000-01-04|  21.1|21.55|20.1|21.55|  7.02372|1412912.0|1.4499989|      6.877053| 0.02132701438231618|\n",
      "|2000-01-05|  21.5| 21.5|20.0| 21.1| 6.877053| 670824.0|      1.5|       7.02372|-0.02088167069115...|\n",
      "|2000-01-06|  21.1| 21.1|21.1| 21.1| 6.877053|      0.0|      0.0|      6.877053|                 0.0|\n",
      "|2000-01-07|  21.5|22.45|20.6| 20.9| 6.811868|1988914.0|1.8500004|      6.877053|-0.00947856520058...|\n",
      "+----------+------+-----+----+-----+---------+---------+---------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_r = df_lag.withColumn('daily_returns',\n",
    "                         calculate_returns(df_lag['Adj Close'], df_lag['prev_day_price']))\n",
    "df_r.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "upm_df = df_r.dropna()\n",
    "upm_df = upm_df.orderBy('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|       daily_returns|\n",
      "+--------------------+\n",
      "| 0.02132701438231618|\n",
      "|-0.02088167069115...|\n",
      "|                 0.0|\n",
      "|-0.00947856520058...|\n",
      "|0.019138746267812527|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "upm_df.select('daily_returns').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = upm_df.agg(spark_functions.min(upm_df.Date)).collect()\n",
    "end_date = upm_df.agg(spark_functions.max(upm_df.Date)).collect()\n",
    "start_date = start_date[0][0]\n",
    "end_date = end_date[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2000, 1, 14)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_length = 10\n",
    "current_end_date = start_date + datetime.timedelta(days=window_length)\n",
    "current_end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = upm_df[(upm_df['Date'] >= start_date ) &\n",
    "      (upm_df['Date'] <= datetime.datetime(2000, 2,1) )][['Date','daily_returns']]\n",
    "r2 = upm_df[(upm_df['Date'] >= start_date ) &\n",
    "      (upm_df['Date'] <= datetime.datetime(2000, 2,1) )]['daily_returns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = upm_df.agg(spark_functions.min(upm_df.Date)).collect()\n",
    "end_date = upm_df.agg(spark_functions.max(upm_df.Date)).collect()\n",
    "start_date = start_date[0][0]\n",
    "end_date = end_date[0][0]\n",
    "batch_container = []\n",
    "batch_container_length = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_end_date = start_date + datetime.timedelta(days=batch_container_length)\n",
    "while current_end_date <= end_date:\n",
    "    #bp()\n",
    "    r = upm_df[(upm_df['Date'] >= start_date) &\n",
    "      (upm_df['Date'] <= current_end_date )][['Date','daily_returns']]\n",
    "    batch_container.append(r.toPandas())\n",
    "    start_date += datetime.timedelta(days=batch_container_length)\n",
    "    current_end_date += datetime.timedelta(days=batch_container_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes regarding following steps with training: \n",
    "At this point, in order to pursue training and testing, it will be necessary to translate the parts of the resulting work to batches, and load to pandas dataframes, so it becomes easy to manipulate the work, and use it to train. \n",
    "\n",
    "Since the current dataset is small enough to work with here, we will just translate it directly to pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>h_l_diff</th>\n",
       "      <th>prev_day_price</th>\n",
       "      <th>daily_returns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>21.1</td>\n",
       "      <td>21.549999</td>\n",
       "      <td>20.100000</td>\n",
       "      <td>21.549999</td>\n",
       "      <td>7.023720</td>\n",
       "      <td>1412912.0</td>\n",
       "      <td>1.449999</td>\n",
       "      <td>6.877053</td>\n",
       "      <td>0.021327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>21.5</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>21.100000</td>\n",
       "      <td>6.877053</td>\n",
       "      <td>670824.0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>7.023720</td>\n",
       "      <td>-0.020882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-06</td>\n",
       "      <td>21.1</td>\n",
       "      <td>21.100000</td>\n",
       "      <td>21.100000</td>\n",
       "      <td>21.100000</td>\n",
       "      <td>6.877053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.877053</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-07</td>\n",
       "      <td>21.5</td>\n",
       "      <td>22.450001</td>\n",
       "      <td>20.600000</td>\n",
       "      <td>20.900000</td>\n",
       "      <td>6.811868</td>\n",
       "      <td>1988914.0</td>\n",
       "      <td>1.850000</td>\n",
       "      <td>6.877053</td>\n",
       "      <td>-0.009479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-01-10</td>\n",
       "      <td>21.5</td>\n",
       "      <td>21.950001</td>\n",
       "      <td>21.200001</td>\n",
       "      <td>21.299999</td>\n",
       "      <td>6.942239</td>\n",
       "      <td>1800878.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>6.811868</td>\n",
       "      <td>0.019139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Open       High        Low      Close  Adj Close     Volume  \\\n",
       "0  2000-01-04  21.1  21.549999  20.100000  21.549999   7.023720  1412912.0   \n",
       "1  2000-01-05  21.5  21.500000  20.000000  21.100000   6.877053   670824.0   \n",
       "2  2000-01-06  21.1  21.100000  21.100000  21.100000   6.877053        0.0   \n",
       "3  2000-01-07  21.5  22.450001  20.600000  20.900000   6.811868  1988914.0   \n",
       "4  2000-01-10  21.5  21.950001  21.200001  21.299999   6.942239  1800878.0   \n",
       "\n",
       "   h_l_diff  prev_day_price  daily_returns  \n",
       "0  1.449999        6.877053       0.021327  \n",
       "1  1.500000        7.023720      -0.020882  \n",
       "2  0.000000        6.877053       0.000000  \n",
       "3  1.850000        6.877053      -0.009479  \n",
       "4  0.750000        6.811868       0.019139  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upm_pd = upm_df.toPandas()\n",
    "upm_pd[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>h_l_diff</th>\n",
       "      <th>prev_day_price</th>\n",
       "      <th>daily_returns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>21.1</td>\n",
       "      <td>21.549999</td>\n",
       "      <td>20.100000</td>\n",
       "      <td>21.549999</td>\n",
       "      <td>7.023720</td>\n",
       "      <td>1412912.0</td>\n",
       "      <td>1.449999</td>\n",
       "      <td>6.877053</td>\n",
       "      <td>0.021327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>21.5</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>21.100000</td>\n",
       "      <td>6.877053</td>\n",
       "      <td>670824.0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>7.023720</td>\n",
       "      <td>-0.020882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-06</td>\n",
       "      <td>21.1</td>\n",
       "      <td>21.100000</td>\n",
       "      <td>21.100000</td>\n",
       "      <td>21.100000</td>\n",
       "      <td>6.877053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.877053</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-07</td>\n",
       "      <td>21.5</td>\n",
       "      <td>22.450001</td>\n",
       "      <td>20.600000</td>\n",
       "      <td>20.900000</td>\n",
       "      <td>6.811868</td>\n",
       "      <td>1988914.0</td>\n",
       "      <td>1.850000</td>\n",
       "      <td>6.877053</td>\n",
       "      <td>-0.009479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-01-10</td>\n",
       "      <td>21.5</td>\n",
       "      <td>21.950001</td>\n",
       "      <td>21.200001</td>\n",
       "      <td>21.299999</td>\n",
       "      <td>6.942239</td>\n",
       "      <td>1800878.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>6.811868</td>\n",
       "      <td>0.019139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Open       High        Low      Close  Adj Close     Volume  \\\n",
       "0  2000-01-04  21.1  21.549999  20.100000  21.549999   7.023720  1412912.0   \n",
       "1  2000-01-05  21.5  21.500000  20.000000  21.100000   6.877053   670824.0   \n",
       "2  2000-01-06  21.1  21.100000  21.100000  21.100000   6.877053        0.0   \n",
       "3  2000-01-07  21.5  22.450001  20.600000  20.900000   6.811868  1988914.0   \n",
       "4  2000-01-10  21.5  21.950001  21.200001  21.299999   6.942239  1800878.0   \n",
       "\n",
       "   h_l_diff  prev_day_price  daily_returns  \n",
       "0  1.449999        6.877053       0.021327  \n",
       "1  1.500000        7.023720      -0.020882  \n",
       "2  0.000000        6.877053       0.000000  \n",
       "3  1.850000        6.877053      -0.009479  \n",
       "4  0.750000        6.811868       0.019139  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upm_pd = upm_pd.dropna()\n",
    "upm_pd[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observations: 2468\n"
     ]
    }
   ],
   "source": [
    "daily_returns = upm_pd['daily_returns'].values\n",
    "window_step = 2\n",
    "window_length = 10\n",
    "observations = int(daily_returns.shape[0]/window_step)\n",
    "training_set = np.zeros([observations, window_length])\n",
    "print('observations: {}'.format(observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.13270144, -2.08816707,  0.        , -0.94785652,  1.91387463,\n",
       "        -4.22531738, -4.90199601,  3.06701701,  2.52563485,  2.195093  ],\n",
       "       [ 0.        , -0.94785652,  1.91387463, -4.22531738, -4.90199601,\n",
       "         3.06701701,  2.52563485,  2.195093  , -0.83527611, -6.37787642],\n",
       "       [ 1.91387463, -4.22531738, -4.90199601,  3.06701701,  2.52563485,\n",
       "         2.195093  , -0.83527611, -6.37787642, -4.37016334,  1.61290005],\n",
       "       [-4.90199601,  3.06701701,  2.52563485,  2.195093  , -0.83527611,\n",
       "        -6.37787642, -4.37016334,  1.61290005, -0.79365313, -2.64002031],\n",
       "       [ 2.52563485,  2.195093  , -0.83527611, -6.37787642, -4.37016334,\n",
       "         1.61290005, -0.79365313, -2.64002031, -1.94467406,  5.02793264]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = 0\n",
    "start_point = 0\n",
    "while row < observations:\n",
    "    current_observations = daily_returns[start_point:start_point + window_length]\n",
    "    if len(current_observations) < window_length:\n",
    "        pass\n",
    "    else:\n",
    "        training_set[row,:] = current_observations\n",
    "    row += 1\n",
    "    start_point += window_step\n",
    "training_set = training_set * 100\n",
    "training_set[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = MinMaxScaler(feature_range=(0,1))\n",
    "#scaler.fit(training_set)\n",
    "#training_set = scaler.transform(training_set)\n",
    "#training_set[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2468, 10)\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(training_set)\n",
    "print(training_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_tensor = torch.tensor(training_set, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_data):\n",
    "        self.output_tensor = torch.tensor(input_data, \n",
    "                                          dtype=torch.float64)\n",
    "        self.output_tensor = self.output_tensor.reshape([self.output_tensor.shape[0],1,\n",
    "                  self.output_tensor.shape[1]])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.output_tensor.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.output_tensor[index, :, :]\n",
    "    \n",
    "    def get_range(self, start_index, end_index):\n",
    "        return self.output_tensor[start_index:end_index,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1967\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset = GeneralDataset(training_set[501:])\n",
    "test_dataset = GeneralDataset(training_set[:500])\n",
    "\n",
    "print(len(training_dataset))\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = torch.utils.data.DataLoader(training_dataset,\n",
    "                                             batch_size=50,\n",
    "                                             shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                             batch_size=50,\n",
    "                                             shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLinear(nn.Module):\n",
    "    def __init__(self, input_size, \n",
    "                 mid_layer_size):\n",
    "        super(EncoderLinear, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, mid_layer_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = x.view(-1, self.fc1.in_features)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, \n",
    "                 mid_layer_size):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.lstm_1 = nn.LSTM(input_size, hidden_layer_size, 1)\n",
    "        self.fc1 = nn.Linear(hidden_layer_size, mid_layer_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #bp()\n",
    "        x = x.float()\n",
    "        lstm_out, (h,c) = self.lstm_1(x)\n",
    "        x = lstm_out.view(lstm_out.shape[0], lstm_out.shape[1] * lstm_out.shape[2])\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLinear(nn.Module):\n",
    "    def __init__(self, mid_layer_size,\n",
    "                 input_size):\n",
    "        super(DecoderLinear, self).__init__()\n",
    "        self.fc1 = nn.Linear(mid_layer_size, input_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = self.fc1(x)\n",
    "        x = x.view(x.shape[0], 1, x.shape[1])\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, mid_layer_size, hidden_layer_size,\n",
    "                 input_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.lstm_1 = nn.LSTM(mid_layer_size, hidden_layer_size, 1)\n",
    "        self.fc1 = nn.Linear(hidden_layer_size, input_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #bp()\n",
    "        x = x.float()\n",
    "        lstm_out, (h,c) = self.lstm_1(x.reshape([x.shape[0],1,x.shape[1]]))\n",
    "        x = lstm_out.view(lstm_out.shape[0], lstm_out.shape[1] * lstm_out.shape[2])\n",
    "        x = self.fc1(x)\n",
    "        x = x.reshape(x.shape[0], 1, x.shape[1])\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape is ok\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1,1, window_length)\n",
    "mid_layer_size = 3\n",
    "hidden_layer_size = 7\n",
    "encoder_test = EncoderLSTM(window_length, hidden_layer_size, mid_layer_size)\n",
    "y = encoder_test(x)\n",
    "#print(y.shape)\n",
    "assert y.shape == torch.Size([1,mid_layer_size]), \"bad shape, y: {}\".format(y.shape)\n",
    "print(\"shape is ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape is ok\n"
     ]
    }
   ],
   "source": [
    "#x = torch.randn(1, 5)\n",
    "decoder_test = DecoderLSTM(mid_layer_size, hidden_layer_size, window_length)\n",
    "#print(y.shape)\n",
    "y = decoder_test(y)\n",
    "assert y.shape == torch.Size([1,1, window_length]), \"bad shape, y: {}\".format(y.shape)\n",
    "print(\"shape is ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderLSTM(\n",
       "  (lstm_1): LSTM(3, 7)\n",
       "  (fc1): Linear(in_features=7, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#encoder = EncoderLinear(window_length, mid_layer_size)\n",
    "encoder = EncoderLSTM(window_length, hidden_layer_size, mid_layer_size)\n",
    "\n",
    "encoder.to(device)\n",
    "\n",
    "#decoder = DecoderLinear(mid_layer_size, window_length)\n",
    "decoder = DecoderLSTM(mid_layer_size, hidden_layer_size, window_length)\n",
    "\n",
    "decoder.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "parameters = list(encoder.parameters()) + list(decoder.parameters())\n",
    "optimizer = optim.Adam(params=parameters,\n",
    "                      lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 30] loss: 4.320635851224264\n",
      "[2, 30] loss: 3.9830880800882977\n",
      "[3, 30] loss: 3.5230156819025678\n",
      "[4, 30] loss: 3.4738654216130573\n",
      "[5, 30] loss: 3.440229304631551\n",
      "[6, 30] loss: 3.161561107635498\n",
      "[7, 30] loss: 3.123292064666748\n",
      "[8, 30] loss: 3.0456082979838053\n",
      "[9, 30] loss: 2.944238305091858\n",
      "[10, 30] loss: 3.095765781402588\n",
      "[11, 30] loss: 3.041571346918742\n",
      "[12, 30] loss: 2.93480589389801\n",
      "[13, 30] loss: 2.987698193391164\n",
      "[14, 30] loss: 3.005099892616272\n",
      "[15, 30] loss: 2.995634587605794\n",
      "[16, 30] loss: 2.968323580423991\n",
      "[17, 30] loss: 2.9729693094889322\n",
      "[18, 30] loss: 2.9375372330347695\n",
      "[19, 30] loss: 2.906225339571635\n",
      "[20, 30] loss: 2.937411340077718\n",
      "[21, 30] loss: 2.964202968279521\n",
      "[22, 30] loss: 2.97530517578125\n",
      "[23, 30] loss: 2.887610101699829\n",
      "[24, 30] loss: 2.8584443092346192\n",
      "[25, 30] loss: 2.9433035333951314\n",
      "[26, 30] loss: 2.909257499376933\n",
      "[27, 30] loss: 2.943731673558553\n",
      "[28, 30] loss: 2.932986768086751\n",
      "[29, 30] loss: 2.9380579312642414\n",
      "[30, 30] loss: 2.9389785051345827\n",
      "[31, 30] loss: 2.8450026432673137\n",
      "[32, 30] loss: 2.9170493682225547\n",
      "[33, 30] loss: 2.904198920726776\n",
      "[34, 30] loss: 2.9172338326772054\n",
      "[35, 30] loss: 2.903648360570272\n",
      "[36, 30] loss: 2.920946196715037\n",
      "[37, 30] loss: 2.8808768590291343\n",
      "[38, 30] loss: 2.8971582730611165\n",
      "[39, 30] loss: 2.8908107837041217\n",
      "[40, 30] loss: 2.8548710227012633\n",
      "[41, 30] loss: 2.8519205331802366\n",
      "[42, 30] loss: 2.951248590151469\n",
      "[43, 30] loss: 2.893205165863037\n",
      "[44, 30] loss: 2.900054649511973\n",
      "[45, 30] loss: 2.8729154109954833\n",
      "[46, 30] loss: 2.927810621261597\n",
      "[47, 30] loss: 2.8465401490529376\n",
      "[48, 30] loss: 2.895592141151428\n",
      "[49, 30] loss: 2.8246617873509723\n",
      "[50, 30] loss: 2.8700278202692666\n",
      "[51, 30] loss: 2.809637951850891\n",
      "[52, 30] loss: 2.803020739555359\n",
      "[53, 30] loss: 2.7943021178245546\n",
      "[54, 30] loss: 2.8420538663864137\n",
      "[55, 30] loss: 2.8693386554718017\n",
      "[56, 30] loss: 2.835383407274882\n",
      "[57, 30] loss: 2.818897763888041\n",
      "[58, 30] loss: 2.7815092126528422\n",
      "[59, 30] loss: 2.8084182421366375\n",
      "[60, 30] loss: 2.8009052435557047\n",
      "[61, 30] loss: 2.7903513590494793\n",
      "[62, 30] loss: 2.7886849919954937\n",
      "[63, 30] loss: 2.8612560590108234\n",
      "[64, 30] loss: 2.813071775436401\n",
      "[65, 30] loss: 2.868930427233378\n",
      "[66, 30] loss: 2.8434834639231363\n",
      "[67, 30] loss: 2.7716931382815044\n",
      "[68, 30] loss: 2.8349236408869425\n",
      "[69, 30] loss: 2.7501192172368367\n",
      "[70, 30] loss: 2.765061291058858\n",
      "[71, 30] loss: 2.757674153645833\n",
      "[72, 30] loss: 2.7390209436416626\n",
      "[73, 30] loss: 2.747797171274821\n",
      "[74, 30] loss: 2.7417191942532857\n",
      "[75, 30] loss: 2.787514646848043\n",
      "[76, 30] loss: 2.752109662691752\n",
      "[77, 30] loss: 2.6499123096466066\n",
      "[78, 30] loss: 2.7198252201080324\n",
      "[79, 30] loss: 2.783790882428487\n",
      "[80, 30] loss: 2.7408836603164675\n",
      "[81, 30] loss: 2.7056793133417765\n",
      "[82, 30] loss: 2.741033418973287\n",
      "[83, 30] loss: 2.7464485724767047\n",
      "[84, 30] loss: 2.7958397110303244\n",
      "[85, 30] loss: 2.734005892276764\n",
      "[86, 30] loss: 2.7146671255429586\n",
      "[87, 30] loss: 2.6457626700401304\n",
      "[88, 30] loss: 2.762248782316844\n",
      "[89, 30] loss: 2.7275960365931193\n",
      "[90, 30] loss: 2.6695831378300983\n",
      "[91, 30] loss: 2.725438066323598\n",
      "[92, 30] loss: 2.657163401444753\n",
      "[93, 30] loss: 2.713893695672353\n",
      "[94, 30] loss: 2.708430294195811\n",
      "[95, 30] loss: 2.674150983492533\n",
      "[96, 30] loss: 2.7115983088811237\n",
      "[97, 30] loss: 2.671055308977763\n",
      "[98, 30] loss: 2.7211870829264324\n",
      "[99, 30] loss: 2.6788179357846578\n",
      "[100, 30] loss: 2.7302435715993245\n",
      "[101, 30] loss: 2.675763789812724\n",
      "[102, 30] loss: 2.629436465104421\n",
      "[103, 30] loss: 2.6982423146565755\n",
      "[104, 30] loss: 2.5898433645566303\n",
      "[105, 30] loss: 2.701759417851766\n",
      "[106, 30] loss: 2.709404488404592\n",
      "[107, 30] loss: 2.6958638191223145\n",
      "[108, 30] loss: 2.68112074136734\n",
      "[109, 30] loss: 2.6884639581044514\n",
      "[110, 30] loss: 2.7107051372528077\n",
      "[111, 30] loss: 2.6699779788653055\n",
      "[112, 30] loss: 2.6846317847569785\n",
      "[113, 30] loss: 2.6362070004145304\n",
      "[114, 30] loss: 2.6283087770144147\n",
      "[115, 30] loss: 2.651995960871379\n",
      "[116, 30] loss: 2.7285966952641805\n",
      "[117, 30] loss: 2.647870449225108\n",
      "[118, 30] loss: 2.6168529192606607\n",
      "[119, 30] loss: 2.700276764233907\n",
      "[120, 30] loss: 2.7305378993352254\n",
      "[121, 30] loss: 2.7098916371663413\n",
      "[122, 30] loss: 2.6673526922861734\n",
      "[123, 30] loss: 2.6431248426437377\n",
      "[124, 30] loss: 2.7460657596588134\n",
      "[125, 30] loss: 2.661400000254313\n",
      "[126, 30] loss: 2.659021627902985\n",
      "[127, 30] loss: 2.6410550038019815\n",
      "[128, 30] loss: 2.598334725697835\n",
      "[129, 30] loss: 2.6485533912976584\n",
      "[130, 30] loss: 2.6351624449094135\n",
      "[131, 30] loss: 2.6670625805854797\n",
      "[132, 30] loss: 2.682483414808909\n",
      "[133, 30] loss: 2.6727750301361084\n",
      "[134, 30] loss: 2.662323307991028\n",
      "[135, 30] loss: 2.65642298857371\n",
      "[136, 30] loss: 2.6346347530682883\n",
      "[137, 30] loss: 2.6882339477539063\n",
      "[138, 30] loss: 2.642944351832072\n",
      "[139, 30] loss: 2.666699202855428\n",
      "[140, 30] loss: 2.6605641722679136\n",
      "[141, 30] loss: 2.6381738424301147\n",
      "[142, 30] loss: 2.6281099915504456\n",
      "[143, 30] loss: 2.5845070083936057\n",
      "[144, 30] loss: 2.6524412751197817\n",
      "[145, 30] loss: 2.6484837849934895\n",
      "[146, 30] loss: 2.6269960085550945\n",
      "[147, 30] loss: 2.62107496658961\n",
      "[148, 30] loss: 2.7083332379659018\n",
      "[149, 30] loss: 2.686112316449483\n",
      "[150, 30] loss: 2.649304966131846\n",
      "[151, 30] loss: 2.6531243642171223\n",
      "[152, 30] loss: 2.6275678793589274\n",
      "[153, 30] loss: 2.655446171760559\n",
      "[154, 30] loss: 2.6571568846702576\n",
      "[155, 30] loss: 2.6255239844322205\n",
      "[156, 30] loss: 2.5873741110165915\n",
      "[157, 30] loss: 2.61671822865804\n",
      "[158, 30] loss: 2.5886049906412762\n",
      "[159, 30] loss: 2.697648513317108\n",
      "[160, 30] loss: 2.6850332736968996\n",
      "[161, 30] loss: 2.644868377844493\n",
      "[162, 30] loss: 2.616677014033\n",
      "[163, 30] loss: 2.614223019282023\n",
      "[164, 30] loss: 2.6137670318285626\n",
      "[165, 30] loss: 2.6354994654655455\n",
      "[166, 30] loss: 2.579333186149597\n",
      "[167, 30] loss: 2.6449015537897744\n",
      "[168, 30] loss: 2.600255807240804\n",
      "[169, 30] loss: 2.6054758151372273\n",
      "[170, 30] loss: 2.6274408141771954\n",
      "[171, 30] loss: 2.663140376408895\n",
      "[172, 30] loss: 2.618978993097941\n",
      "[173, 30] loss: 2.6939981818199157\n",
      "[174, 30] loss: 2.5118085225423177\n",
      "[175, 30] loss: 2.625041302045186\n",
      "[176, 30] loss: 2.6705009937286377\n",
      "[177, 30] loss: 2.6382541100184125\n",
      "[178, 30] loss: 2.605839411417643\n",
      "[179, 30] loss: 2.6455855051676433\n",
      "[180, 30] loss: 2.5765395283699037\n",
      "[181, 30] loss: 2.6046206315358478\n",
      "[182, 30] loss: 2.6037132660547893\n",
      "[183, 30] loss: 2.630213956038157\n",
      "[184, 30] loss: 2.6897016485532124\n",
      "[185, 30] loss: 2.5918024222056073\n",
      "[186, 30] loss: 2.653929869333903\n",
      "[187, 30] loss: 2.6059473077456157\n",
      "[188, 30] loss: 2.5878510316212973\n",
      "[189, 30] loss: 2.6254462281862896\n",
      "[190, 30] loss: 2.605351948738098\n",
      "[191, 30] loss: 2.6111035982767743\n",
      "[192, 30] loss: 2.6186192949612934\n",
      "[193, 30] loss: 2.5623952666918437\n",
      "[194, 30] loss: 2.615024507045746\n",
      "[195, 30] loss: 2.601351594924927\n",
      "[196, 30] loss: 2.6058365305264792\n",
      "[197, 30] loss: 2.6729394435882567\n",
      "[198, 30] loss: 2.6581666469573975\n",
      "[199, 30] loss: 2.6238508184750873\n",
      "[200, 30] loss: 2.57928812901179\n",
      "[201, 30] loss: 2.575701419512431\n",
      "[202, 30] loss: 2.544780381520589\n",
      "[203, 30] loss: 2.5887910962104796\n",
      "[204, 30] loss: 2.55483029683431\n",
      "[205, 30] loss: 2.6025477687517804\n",
      "[206, 30] loss: 2.5887998143831887\n",
      "[207, 30] loss: 2.5739838520685834\n",
      "[208, 30] loss: 2.581062642733256\n",
      "[209, 30] loss: 2.579266142845154\n",
      "[210, 30] loss: 2.611531647046407\n",
      "[211, 30] loss: 2.544040242830912\n",
      "[212, 30] loss: 2.58911376396815\n",
      "[213, 30] loss: 2.576631804307302\n",
      "[214, 30] loss: 2.5272953748703\n",
      "[215, 30] loss: 2.6062758684158327\n",
      "[216, 30] loss: 2.605150071779887\n",
      "[217, 30] loss: 2.553632978598277\n",
      "[218, 30] loss: 2.6286123275756834\n",
      "[219, 30] loss: 2.590547815958659\n",
      "[220, 30] loss: 2.5976488987604776\n",
      "[221, 30] loss: 2.644979004065196\n",
      "[222, 30] loss: 2.5464333335558575\n",
      "[223, 30] loss: 2.6213583747545877\n",
      "[224, 30] loss: 2.5369182268778485\n",
      "[225, 30] loss: 2.5912163257598877\n",
      "[226, 30] loss: 2.644575564066569\n",
      "[227, 30] loss: 2.5442291418711345\n",
      "[228, 30] loss: 2.570340637365977\n",
      "[229, 30] loss: 2.6222389618555706\n",
      "[230, 30] loss: 2.5778589844703674\n",
      "[231, 30] loss: 2.521120270093282\n",
      "[232, 30] loss: 2.603431510925293\n",
      "[233, 30] loss: 2.632706940174103\n",
      "[234, 30] loss: 2.596118740240733\n",
      "[235, 30] loss: 2.6510693311691282\n",
      "[236, 30] loss: 2.5427280982335407\n",
      "[237, 30] loss: 2.6362011710802715\n",
      "[238, 30] loss: 2.5360243042310078\n",
      "[239, 30] loss: 2.5611419518788656\n",
      "[240, 30] loss: 2.613426641623179\n",
      "[241, 30] loss: 2.5828616619110107\n",
      "[242, 30] loss: 2.644488231341044\n",
      "[243, 30] loss: 2.548168937365214\n",
      "[244, 30] loss: 2.553845691680908\n",
      "[245, 30] loss: 2.5825684388478596\n",
      "[246, 30] loss: 2.604106386502584\n",
      "[247, 30] loss: 2.531297822793325\n",
      "[248, 30] loss: 2.594979468981425\n",
      "[249, 30] loss: 2.5344305594762164\n",
      "[250, 30] loss: 2.5015042344729106\n",
      "[251, 30] loss: 2.578385353088379\n",
      "[252, 30] loss: 2.631521797180176\n",
      "[253, 30] loss: 2.5612624724706015\n",
      "[254, 30] loss: 2.526391792297363\n",
      "[255, 30] loss: 2.5795719226201377\n",
      "[256, 30] loss: 2.6008028268814085\n",
      "[257, 30] loss: 2.5594489137331644\n",
      "[258, 30] loss: 2.5258682211240133\n",
      "[259, 30] loss: 2.5454177021980287\n",
      "[260, 30] loss: 2.51490007241567\n",
      "[261, 30] loss: 2.581766970952352\n",
      "[262, 30] loss: 2.6280869960784914\n",
      "[263, 30] loss: 2.5703920046488444\n",
      "[264, 30] loss: 2.5827961285909016\n",
      "[265, 30] loss: 2.54904682636261\n",
      "[266, 30] loss: 2.5558810671170553\n",
      "[267, 30] loss: 2.6195960640907288\n",
      "[268, 30] loss: 2.5759608070055644\n",
      "[269, 30] loss: 2.5785186529159545\n",
      "[270, 30] loss: 2.5900928020477294\n",
      "[271, 30] loss: 2.5435295859972635\n",
      "[272, 30] loss: 2.5768593271573383\n",
      "[273, 30] loss: 2.5416196982065835\n",
      "[274, 30] loss: 2.5486714204152423\n",
      "[275, 30] loss: 2.553260934352875\n",
      "[276, 30] loss: 2.622558832168579\n",
      "[277, 30] loss: 2.5557778676350913\n",
      "[278, 30] loss: 2.5469752033551534\n",
      "[279, 30] loss: 2.5562132199605307\n",
      "[280, 30] loss: 2.524976885318756\n",
      "[281, 30] loss: 2.5791901191075643\n",
      "[282, 30] loss: 2.6266339858373007\n",
      "[283, 30] loss: 2.5217556158701577\n",
      "[284, 30] loss: 2.58455882469813\n",
      "[285, 30] loss: 2.5030126730600992\n",
      "[286, 30] loss: 2.552586809794108\n",
      "[287, 30] loss: 2.5409482598304747\n",
      "[288, 30] loss: 2.558974285920461\n",
      "[289, 30] loss: 2.5694404284159345\n",
      "[290, 30] loss: 2.5598241090774536\n",
      "[291, 30] loss: 2.54453337987264\n",
      "[292, 30] loss: 2.5419689297676085\n",
      "[293, 30] loss: 2.5296632726987203\n",
      "[294, 30] loss: 2.5356614192326865\n",
      "[295, 30] loss: 2.526605443159739\n",
      "[296, 30] loss: 2.5511537313461305\n",
      "[297, 30] loss: 2.5853476484616595\n",
      "[298, 30] loss: 2.6108316580454507\n",
      "[299, 30] loss: 2.499771237373352\n",
      "[300, 30] loss: 2.5721057057380676\n",
      "[301, 30] loss: 2.5360897461573284\n",
      "[302, 30] loss: 2.5487669308980307\n",
      "[303, 30] loss: 2.5734379450480143\n",
      "[304, 30] loss: 2.4715599338213603\n",
      "[305, 30] loss: 2.519022516409556\n",
      "[306, 30] loss: 2.5294349273045857\n",
      "[307, 30] loss: 2.4943369030952454\n",
      "[308, 30] loss: 2.5697779417037965\n",
      "[309, 30] loss: 2.5398826162020365\n",
      "[310, 30] loss: 2.558062485853831\n",
      "[311, 30] loss: 2.5115185976028442\n",
      "[312, 30] loss: 2.4868196407953898\n",
      "[313, 30] loss: 2.478792218367259\n",
      "[314, 30] loss: 2.5597999771436055\n",
      "[315, 30] loss: 2.562095864613851\n",
      "[316, 30] loss: 2.514277199904124\n",
      "[317, 30] loss: 2.554067877928416\n",
      "[318, 30] loss: 2.5157357970873515\n",
      "[319, 30] loss: 2.6073596119880675\n",
      "[320, 30] loss: 2.5282811443010966\n",
      "[321, 30] loss: 2.5443408926328024\n",
      "[322, 30] loss: 2.5178115566571555\n",
      "[323, 30] loss: 2.604491897424062\n",
      "[324, 30] loss: 2.5184011340141295\n",
      "[325, 30] loss: 2.5553460836410524\n",
      "[326, 30] loss: 2.5909399231274923\n",
      "[327, 30] loss: 2.567721656958262\n",
      "[328, 30] loss: 2.517845030625661\n",
      "[329, 30] loss: 2.4976644992828367\n",
      "[330, 30] loss: 2.5702255328496295\n",
      "[331, 30] loss: 2.541150947411855\n",
      "[332, 30] loss: 2.529071299235026\n",
      "[333, 30] loss: 2.561426536242167\n",
      "[334, 30] loss: 2.5023553133010865\n",
      "[335, 30] loss: 2.526713256041209\n",
      "[336, 30] loss: 2.4934951345125835\n",
      "[337, 30] loss: 2.611019229888916\n",
      "[338, 30] loss: 2.502878654003143\n",
      "[339, 30] loss: 2.534540323416392\n",
      "[340, 30] loss: 2.5612447261810303\n",
      "[341, 30] loss: 2.573315413792928\n",
      "[342, 30] loss: 2.5308519045511884\n",
      "[343, 30] loss: 2.5495579560597736\n",
      "[344, 30] loss: 2.4770249366760253\n",
      "[345, 30] loss: 2.5306602080663043\n",
      "[346, 30] loss: 2.5018162608146666\n",
      "[347, 30] loss: 2.4905644178390505\n",
      "[348, 30] loss: 2.511192278067271\n",
      "[349, 30] loss: 2.5046225945154825\n",
      "[350, 30] loss: 2.4664727012316385\n",
      "[351, 30] loss: 2.54027715921402\n",
      "[352, 30] loss: 2.502148417631785\n",
      "[353, 30] loss: 2.473672564824422\n",
      "[354, 30] loss: 2.5459391911824545\n",
      "[355, 30] loss: 2.5636850436528524\n",
      "[356, 30] loss: 2.4624041438102724\n",
      "[357, 30] loss: 2.4939427653948467\n",
      "[358, 30] loss: 2.581882850329081\n",
      "[359, 30] loss: 2.5728342731793723\n",
      "[360, 30] loss: 2.5786858995755515\n",
      "[361, 30] loss: 2.5593011061350506\n",
      "[362, 30] loss: 2.540373742580414\n",
      "[363, 30] loss: 2.5283326705296836\n",
      "[364, 30] loss: 2.4849551200866697\n",
      "[365, 30] loss: 2.573512438933055\n",
      "[366, 30] loss: 2.5216102242469787\n",
      "[367, 30] loss: 2.5310603737831117\n",
      "[368, 30] loss: 2.528857664267222\n",
      "[369, 30] loss: 2.5249372800191243\n",
      "[370, 30] loss: 2.5025630633036298\n",
      "[371, 30] loss: 2.5686277031898497\n",
      "[372, 30] loss: 2.4934624671936034\n",
      "[373, 30] loss: 2.544280695915222\n",
      "[374, 30] loss: 2.549074482917786\n",
      "[375, 30] loss: 2.504701534907023\n",
      "[376, 30] loss: 2.5223819812138877\n",
      "[377, 30] loss: 2.584061920642853\n",
      "[378, 30] loss: 2.4747002442677815\n",
      "[379, 30] loss: 2.523015081882477\n",
      "[380, 30] loss: 2.525448290506999\n",
      "[381, 30] loss: 2.497752829392751\n",
      "[382, 30] loss: 2.5277923464775087\n",
      "[383, 30] loss: 2.551536564032237\n",
      "[384, 30] loss: 2.541065772374471\n",
      "[385, 30] loss: 2.521104403336843\n",
      "[386, 30] loss: 2.5425068656603496\n",
      "[387, 30] loss: 2.516182374954224\n",
      "[388, 30] loss: 2.5106897830963133\n",
      "[389, 30] loss: 2.5175219774246216\n",
      "[390, 30] loss: 2.518303720156352\n",
      "[391, 30] loss: 2.5095597306887307\n",
      "[392, 30] loss: 2.5148258606592813\n",
      "[393, 30] loss: 2.5000679890314736\n",
      "[394, 30] loss: 2.527598138650258\n",
      "[395, 30] loss: 2.539380657672882\n",
      "[396, 30] loss: 2.479059425989787\n",
      "[397, 30] loss: 2.4798395077387494\n",
      "[398, 30] loss: 2.47300980091095\n",
      "[399, 30] loss: 2.529570396741231\n",
      "[400, 30] loss: 2.490943161646525\n",
      "[401, 30] loss: 2.4921250502268473\n",
      "[402, 30] loss: 2.4668681621551514\n",
      "[403, 30] loss: 2.5099713961283365\n",
      "[404, 30] loss: 2.5060601433118186\n",
      "[405, 30] loss: 2.53840305407842\n",
      "[406, 30] loss: 2.509741497039795\n",
      "[407, 30] loss: 2.5341362913449603\n",
      "[408, 30] loss: 2.5485477685928344\n",
      "[409, 30] loss: 2.5348383943239847\n",
      "[410, 30] loss: 2.479349152247111\n",
      "[411, 30] loss: 2.518610207239787\n",
      "[412, 30] loss: 2.490366514523824\n",
      "[413, 30] loss: 2.451700949668884\n",
      "[414, 30] loss: 2.489268596967061\n",
      "[415, 30] loss: 2.4569618384043377\n",
      "[416, 30] loss: 2.4003907958666484\n",
      "[417, 30] loss: 2.490850563844045\n",
      "[418, 30] loss: 2.4708582162857056\n",
      "[419, 30] loss: 2.474934001763662\n",
      "[420, 30] loss: 2.4994452754656473\n",
      "[421, 30] loss: 2.522303517659505\n",
      "[422, 30] loss: 2.509961398442586\n",
      "[423, 30] loss: 2.449986755847931\n",
      "[424, 30] loss: 2.543126130104065\n",
      "[425, 30] loss: 2.5160367687543235\n",
      "[426, 30] loss: 2.503384900093079\n",
      "[427, 30] loss: 2.4243901054064434\n",
      "[428, 30] loss: 2.5266764163970947\n",
      "[429, 30] loss: 2.5477577368418376\n",
      "[430, 30] loss: 2.4984458684921265\n",
      "[431, 30] loss: 2.4969321211179096\n",
      "[432, 30] loss: 2.463371471563975\n",
      "[433, 30] loss: 2.451112139225006\n",
      "[434, 30] loss: 2.449536120891571\n",
      "[435, 30] loss: 2.426058316230774\n",
      "[436, 30] loss: 2.5000195701917014\n",
      "[437, 30] loss: 2.4813605109850565\n",
      "[438, 30] loss: 2.5216898957888287\n",
      "[439, 30] loss: 2.5096643368403115\n",
      "[440, 30] loss: 2.520264478524526\n",
      "[441, 30] loss: 2.4455753684043886\n",
      "[442, 30] loss: 2.382531213760376\n",
      "[443, 30] loss: 2.523068388303121\n",
      "[444, 30] loss: 2.5228312412897744\n",
      "[445, 30] loss: 2.5058805187543234\n",
      "[446, 30] loss: 2.565910991032918\n",
      "[447, 30] loss: 2.4806499044100443\n",
      "[448, 30] loss: 2.5138076146443686\n",
      "[449, 30] loss: 2.484473180770874\n",
      "[450, 30] loss: 2.4385136365890503\n",
      "[451, 30] loss: 2.4370890816052753\n",
      "[452, 30] loss: 2.519366908073425\n",
      "[453, 30] loss: 2.475802524884542\n",
      "[454, 30] loss: 2.495351958274841\n",
      "[455, 30] loss: 2.4955268343289694\n",
      "[456, 30] loss: 2.4862215836842854\n",
      "[457, 30] loss: 2.532472252845764\n",
      "[458, 30] loss: 2.4769940773646035\n",
      "[459, 30] loss: 2.5012258211771647\n",
      "[460, 30] loss: 2.472587502002716\n",
      "[461, 30] loss: 2.5283327102661133\n",
      "[462, 30] loss: 2.4909901062647504\n",
      "[463, 30] loss: 2.505123539765676\n",
      "[464, 30] loss: 2.5072588602701824\n",
      "[465, 30] loss: 2.4589911381403606\n",
      "[466, 30] loss: 2.475282899538676\n",
      "[467, 30] loss: 2.502987281481425\n",
      "[468, 30] loss: 2.466701889038086\n",
      "[469, 30] loss: 2.4513876914978026\n",
      "[470, 30] loss: 2.487185843785604\n",
      "[471, 30] loss: 2.4712199052174886\n",
      "[472, 30] loss: 2.500026603539785\n",
      "[473, 30] loss: 2.4332033435503644\n",
      "[474, 30] loss: 2.5130531430244445\n",
      "[475, 30] loss: 2.483569304148356\n",
      "[476, 30] loss: 2.4815340876579284\n",
      "[477, 30] loss: 2.4538912057876585\n",
      "[478, 30] loss: 2.5364948829015095\n",
      "[479, 30] loss: 2.5046826004981995\n",
      "[480, 30] loss: 2.547532629966736\n",
      "[481, 30] loss: 2.4845647692680357\n",
      "[482, 30] loss: 2.471060311794281\n",
      "[483, 30] loss: 2.467318399747213\n",
      "[484, 30] loss: 2.532936199506124\n",
      "[485, 30] loss: 2.4634859641393025\n",
      "[486, 30] loss: 2.4281630635261537\n",
      "[487, 30] loss: 2.5659951329231263\n",
      "[488, 30] loss: 2.5601423223813375\n",
      "[489, 30] loss: 2.4847421169281008\n",
      "[490, 30] loss: 2.4991187969843547\n",
      "[491, 30] loss: 2.4924834251403807\n",
      "[492, 30] loss: 2.500606644153595\n",
      "[493, 30] loss: 2.4988062222798666\n",
      "[494, 30] loss: 2.4857882062594094\n",
      "[495, 30] loss: 2.5131226340929667\n",
      "[496, 30] loss: 2.5142681002616882\n",
      "[497, 30] loss: 2.4897770524024962\n",
      "[498, 30] loss: 2.470307266712189\n",
      "[499, 30] loss: 2.5066725889841717\n",
      "[500, 30] loss: 2.5106528480847676\n",
      "finished training\n"
     ]
    }
   ],
   "source": [
    "print_interval = 30\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for index, batch in enumerate(training_loader):\n",
    "        #bp()\n",
    "        batch = batch.float()\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        encoder_output = encoder(batch)\n",
    "        decoder_output = decoder(encoder_output)\n",
    "        \n",
    "        loss = criterion(decoder_output, batch)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        #print information\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if index % print_interval == (print_interval - 1):\n",
    "            print(\"[{}, {}] loss: {}\".format(epoch + 1,\n",
    "                                            index + 1, \n",
    "                                            running_loss / print_interval))\n",
    "            running_loss = 0.0\n",
    "print(\"finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1851, device='cuda:0')\n",
      "tensor([[-0.7074, -2.5824, -8.1353, -2.2885, -4.7862],\n",
      "        [ 0.4031,  2.0561, -0.5560, -0.8107,  0.2442]])\n",
      "tensor(21.5365)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for index, batch in enumerate(test_loader):\n",
    "        #bp()\n",
    "        batch = batch.float()\n",
    "        batch = batch.to(device)\n",
    "        encoder_output = encoder(batch)\n",
    "        decoder_output = decoder(encoder_output)\n",
    "        \n",
    "        loss = criterion(decoder_output, batch)\n",
    "        print(loss)\n",
    "        \n",
    "        batch_example = torch.zeros([2,5])\n",
    "        batch_example[0,:] = batch[0][0][:5]\n",
    "        batch_example[1,:] = decoder_output[0][0][:5]\n",
    "        #batch_example = torch.log10(batch_example)\n",
    "        print(batch_example)\n",
    "        print(F.mse_loss(batch_example[0],batch_example[1]))\n",
    "        if index >= 0:\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0343, device='cuda:0')\n",
      "tensor([[-3.1861, -0.1732,  0.9832, -2.2337, -1.2302],\n",
      "        [-0.1516, -0.9208,  0.9052,  0.1902, -2.0160]])\n",
      "tensor(3.2532)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for index, batch in enumerate(training_loader):\n",
    "        #bp()\n",
    "        batch = batch.float()\n",
    "        batch = batch.to(device)\n",
    "        encoder_output = encoder(batch)\n",
    "        decoder_output = decoder(encoder_output)\n",
    "        \n",
    "        loss = criterion(decoder_output, batch)\n",
    "        print(loss)\n",
    "        \n",
    "        batch_example = torch.zeros([2,5])\n",
    "        batch_example[0,:] = batch[0][0][:5]\n",
    "        batch_example[1,:] = decoder_output[0][0][:5]\n",
    "        #batch_example = torch.log10(batch_example)\n",
    "        print(batch_example)\n",
    "        print(F.mse_loss(batch_example[0],batch_example[1]))\n",
    "        if index >= 0:\n",
    "            break\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
